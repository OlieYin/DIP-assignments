{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the polygon state\n",
    "def initialize_polygon():\n",
    "    \"\"\"\n",
    "    Initializes the polygon state.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'points' and 'closed' status.\n",
    "    \"\"\"\n",
    "    return {'points': [], 'closed': False}\n",
    "\n",
    "# Add a point to the polygon when the user clicks on the image\n",
    "def add_point(img_original, polygon_state, evt: gr.SelectData):\n",
    "    \"\"\"\n",
    "    Adds a point to the polygon based on user click event.\n",
    "\n",
    "    Args:\n",
    "        img_original (PIL.Image): The original image.\n",
    "        polygon_state (dict): The current state of the polygon.\n",
    "        evt (gr.SelectData): The click event data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated image with polygon and updated polygon state.\n",
    "    \"\"\"\n",
    "    if polygon_state['closed']:\n",
    "        return img_original, polygon_state  # Do not add points if polygon is closed\n",
    "\n",
    "    x, y = evt.index\n",
    "    polygon_state['points'].append((x, y))\n",
    "\n",
    "    img_with_poly = img_original.copy()\n",
    "    draw = ImageDraw.Draw(img_with_poly)\n",
    "\n",
    "    # Draw lines between points\n",
    "    if len(polygon_state['points']) > 1:\n",
    "        draw.line(polygon_state['points'], fill='red', width=2)\n",
    "\n",
    "    # Draw points\n",
    "    for point in polygon_state['points']:\n",
    "        draw.ellipse((point[0]-3, point[1]-3, point[0]+3, point[1]+3), fill='blue')\n",
    "\n",
    "    return img_with_poly, polygon_state\n",
    "\n",
    "# Close the polygon when the user clicks the \"Close Polygon\" button\n",
    "def close_polygon(img_original, polygon_state):\n",
    "    \"\"\"\n",
    "    Closes the polygon if there are at least three points.\n",
    "\n",
    "    Args:\n",
    "        img_original (PIL.Image): The original image.\n",
    "        polygon_state (dict): The current state of the polygon.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated image with closed polygon and updated polygon state.\n",
    "    \"\"\"\n",
    "    if not polygon_state['closed'] and len(polygon_state['points']) > 2:\n",
    "        polygon_state['closed'] = True\n",
    "        img_with_poly = img_original.copy()\n",
    "        draw = ImageDraw.Draw(img_with_poly)\n",
    "        draw.polygon(polygon_state['points'], outline='red')\n",
    "        return img_with_poly, polygon_state\n",
    "    else:\n",
    "        return img_original, polygon_state\n",
    "\n",
    "# Update the background image by drawing the shifted polygon on it\n",
    "def update_background(background_image_original, polygon_state, dx, dy):\n",
    "    \"\"\"\n",
    "    Updates the background image by drawing the shifted polygon on it.\n",
    "\n",
    "    Args:\n",
    "        background_image_original (PIL.Image): The original background image.\n",
    "        polygon_state (dict): The current state of the polygon.\n",
    "        dx (int): Horizontal offset.\n",
    "        dy (int): Vertical offset.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: The updated background image with the polygon overlay.\n",
    "    \"\"\"\n",
    "    if background_image_original is None:\n",
    "        return None\n",
    "\n",
    "    if polygon_state['closed']:\n",
    "        img_with_poly = background_image_original.copy()\n",
    "        draw = ImageDraw.Draw(img_with_poly)\n",
    "        shifted_points = [(x + dx, y + dy) for x, y in polygon_state['points']]\n",
    "        draw.polygon(shifted_points, outline='red')\n",
    "        return img_with_poly\n",
    "    else:\n",
    "        return background_image_original\n",
    "\n",
    "# Create a binary mask from polygon points\n",
    "def create_mask_from_points(points, img_h, img_w):\n",
    "    \"\"\"\n",
    "    Creates a binary mask from the given polygon points.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Polygon points of shape (n, 2).\n",
    "        img_h (int): Image height.\n",
    "        img_w (int): Image width.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Binary mask of shape (img_h, img_w).\n",
    "    \"\"\"\n",
    "    mask = np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "    ### FILL: Obtain Mask from Polygon Points. \n",
    "    ### 0 indicates outside the Polygon.\n",
    "    ### 255 indicates inside the Polygon.\n",
    "\n",
    "    # Ensure points are in the correct shape (n, 1, 2) for fillPoly\n",
    "    points = points.reshape((-1, 1, 2)).astype(np.int32)\n",
    "\n",
    "    # Fill the polygon on the mask with 255 (white)\n",
    "    cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_mask_boundary(mask_expanded):\n",
    "    \"\"\"\n",
    "    获取掩膜的内部边界像素（布尔掩码）。\n",
    "    Args:\n",
    "        mask_expanded (torch.Tensor): 输入掩膜张量，形状为 (1, 3, H, W)。\n",
    "    Returns:\n",
    "        torch.Tensor: 标记内部边界像素的布尔掩码，形状为 (1, 3, H, W)。\n",
    "    \"\"\"\n",
    "    # 将掩膜转换为单通道形式 (1, 1, H, W)\n",
    "    mask_single_channel = mask_expanded[:, :1, :, :]  # 取第一个通道\n",
    "\n",
    "    # 使用 3x3 卷积核进行收缩操作 (erosion)\n",
    "    kernel = torch.ones((1, 1, 3, 3), dtype=torch.float32, device=mask_expanded.device)\n",
    "    eroded_mask = F.conv2d(mask_single_channel.float(), kernel, padding=1) == 9  # 完全包含 9 个像素点时保留\n",
    "\n",
    "    # 计算内部边界：原掩膜 - 收缩后的掩膜\n",
    "    boundary_mask = mask_single_channel.bool() & ~eroded_mask\n",
    "\n",
    "    # 将内部边界掩膜扩展回到 (1, 3, H, W)\n",
    "    boundary_mask = boundary_mask.expand(-1, 3, -1, -1)\n",
    "\n",
    "    return boundary_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Laplacian loss between the foreground and blended image\n",
    "def cal_laplacian_loss(foreground_img, foreground_mask, blended_img, background_mask):\n",
    "    \"\"\"\n",
    "    Computes the Laplacian loss between the foreground and blended images within the masks.\n",
    "\n",
    "    Args:\n",
    "        foreground_img (torch.Tensor): Foreground image tensor.\n",
    "        foreground_mask (torch.Tensor): Foreground mask tensor.\n",
    "        blended_img (torch.Tensor): Blended image tensor.\n",
    "        background_mask (torch.Tensor): Background mask tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed Laplacian loss.\n",
    "    \"\"\"\n",
    "    loss = torch.tensor(0.0, device=foreground_img.device)\n",
    "    ### FILL: Compute Laplacian Loss with https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html.\n",
    "    ### Note: The loss is computed within the masks.\n",
    "\n",
    "    laplacian_kernel = torch.tensor([[[[0, -1, 0], \n",
    "                                       [-1, 4, -1], \n",
    "                                       [0, -1, 0]]]], dtype=torch.float32, device=foreground_img.device)\n",
    "\n",
    "    laplacian_kernel = laplacian_kernel.repeat(3,1,1,1)\n",
    "\n",
    "    # Apply the masks to the images\n",
    "    f_tude = torch.zeros_like(blended_img, device=foreground_img.device)\n",
    "    blended_area = torch.zeros_like(background_mask, device=foreground_img.device)\n",
    "    foreground_mask_expand = foreground_mask.bool().expand(-1, 3, -1, -1)\n",
    "    background_mask_expand = background_mask.bool().expand(-1, 3, -1, -1)\n",
    "    \n",
    "    f_tude[background_mask_expand] = blended_img[background_mask_expand] - foreground_img[foreground_mask_expand]\n",
    "    # Compute the difference between masked images (g_tude)\n",
    "\n",
    "\n",
    "    # Apply Laplacian convolution on the difference\n",
    "    laplacian_f_tude = F.conv2d(f_tude, laplacian_kernel, padding=1, groups=foreground_img.shape[1])\n",
    "\n",
    "    loss = torch.abs(laplacian_f_tude).mean()\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Perform Poisson image blending\n",
    "def blending(foreground_image_original, background_image_original, dx, dy, polygon_state):\n",
    "    \"\"\"\n",
    "    Blends the foreground polygon area onto the background image using Poisson blending.\n",
    "\n",
    "    Args:\n",
    "        foreground_image_original (PIL.Image): The original foreground image.\n",
    "        background_image_original (PIL.Image): The original background image.\n",
    "        dx (int): Horizontal offset.\n",
    "        dy (int): Vertical offset.\n",
    "        polygon_state (dict): The current state of the polygon.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The blended image as a numpy array.\n",
    "    \"\"\"\n",
    "    if not polygon_state['closed'] or background_image_original is None or foreground_image_original is None:\n",
    "        return background_image_original  # Return original background if conditions are not met\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    foreground_np = np.array(foreground_image_original)\n",
    "    background_np = np.array(background_image_original)\n",
    "\n",
    "    # Get polygon points and shift them by dx and dy\n",
    "    foreground_polygon_points = np.array(polygon_state['points']).astype(np.int64)\n",
    "    background_polygon_points = foreground_polygon_points + np.array([int(dx), int(dy)]).reshape(1, 2)\n",
    "\n",
    "    # Create masks from polygon points\n",
    "    foreground_mask = create_mask_from_points(foreground_polygon_points, foreground_np.shape[0], foreground_np.shape[1])\n",
    "    background_mask = create_mask_from_points(background_polygon_points, background_np.shape[0], background_np.shape[1])\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # Using CPU will be slow\n",
    "    fg_img_tensor = torch.from_numpy(foreground_np).to(device).permute(2, 0, 1).unsqueeze(0).float() / 255.\n",
    "    bg_img_tensor = torch.from_numpy(background_np).to(device).permute(2, 0, 1).unsqueeze(0).float() / 255.\n",
    "    fg_mask_tensor = torch.from_numpy(foreground_mask).to(device).unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "    bg_mask_tensor = torch.from_numpy(background_mask).to(device).unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "\n",
    "    # Initialize blended image\n",
    "    blended_img = bg_img_tensor.clone()\n",
    "    mask_expanded = bg_mask_tensor.bool().expand(-1, 3, -1, -1)\n",
    "    blended_img[mask_expanded] = blended_img[mask_expanded] * 0.9 + fg_img_tensor[fg_mask_tensor.bool().expand(-1, 3, -1, -1)] * 0.1\n",
    "    blended_img.requires_grad = True\n",
    "\n",
    "    boundary_mask = get_mask_boundary(mask_expanded)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([blended_img], lr=5e-3)\n",
    "\n",
    "    # Optimization loop\n",
    "    iter_num = 10000\n",
    "    for step in range(iter_num):\n",
    "        blended_img_for_loss = blended_img.detach() * (1. - bg_mask_tensor) + blended_img * bg_mask_tensor  # Only blending in the mask region\n",
    "\n",
    "        loss = cal_laplacian_loss(fg_img_tensor, fg_mask_tensor, blended_img_for_loss, bg_mask_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f'Optimize step: {step}, Laplacian distance loss: {loss.item()}')\n",
    "\n",
    "        if step == (iter_num // 2): ### decrease learning rate at the half step\n",
    "            optimizer.param_groups[0]['lr'] *= 0.1\n",
    "\n",
    "        blended_img.data[boundary_mask] = bg_img_tensor.detach().clone().data[boundary_mask]\n",
    "    # Convert result back to numpy array\n",
    "    result = torch.clamp(blended_img.detach(), 0, 1).cpu().permute(0, 2, 3, 1).squeeze().numpy() * 255\n",
    "    result = result.astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "# Helper function to close the polygon and reset dx\n",
    "def close_polygon_and_reset_dx(img_original, polygon_state, dx, dy, background_image_original):\n",
    "    \"\"\"\n",
    "    Closes the polygon, resets dx to 0, and updates the background image.\n",
    "\n",
    "    Args:\n",
    "        img_original (PIL.Image): The original image.\n",
    "        polygon_state (dict): The current state of the polygon.\n",
    "        dx (int): Horizontal offset.\n",
    "        dy (int): Vertical offset.\n",
    "        background_image_original (PIL.Image): The original background image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated image with polygon, updated polygon state, updated background image, and reset dx value.\n",
    "    \"\"\"\n",
    "    # Close polygon\n",
    "    img_with_poly, updated_polygon_state = close_polygon(img_original, polygon_state)\n",
    "\n",
    "    # Reset dx value to 0\n",
    "    new_dx = gr.update(value=0)\n",
    "\n",
    "    # Update background image\n",
    "    updated_background = update_background(background_image_original, updated_polygon_state, 0, dy)\n",
    "    return img_with_poly, updated_polygon_state, updated_background, new_dx\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks(title=\"Poisson Image Blending\", css=\"\"\"\n",
    "    body {\n",
    "        background-color: #1e1e1e;\n",
    "        color: #ffffff;\n",
    "    }\n",
    "    .gr-button {\n",
    "        font-size: 1em;\n",
    "        padding: 0.75em 1.5em;\n",
    "        border-radius: 8px;\n",
    "        background-color: #6200ee;\n",
    "        color: #ffffff;\n",
    "        border: none;\n",
    "    }\n",
    "    .gr-button:hover {\n",
    "        background-color: #3700b3;\n",
    "    }\n",
    "    .gr-slider input[type=range] {\n",
    "        accent-color: #03dac6;\n",
    "    }\n",
    "    .gr-text, .gr-markdown {\n",
    "        font-size: 1.1em;\n",
    "    }\n",
    "    .gr-markdown h1, .gr-markdown h2, .gr-markdown h3 {\n",
    "        color: #bb86fc;\n",
    "    }\n",
    "    .gr-input, .gr-output {\n",
    "        background-color: #2c2c2c;\n",
    "        border: 1px solid #3c3c3c;\n",
    "    }\n",
    "\"\"\") as demo:\n",
    "    # Initialize states\n",
    "    polygon_state = gr.State(initialize_polygon())\n",
    "    background_image_original = gr.State(value=None)\n",
    "\n",
    "    # Title and description\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Poisson Image Blending</h1>\")\n",
    "    gr.Markdown(\"<p style='text-align: center; font-size: 1.2em;'>Blend a selected area from a foreground image onto a background image with adjustable positions.</p>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Foreground Image\")\n",
    "            foreground_image_original = gr.Image(\n",
    "                label=\"\", type=\"pil\", interactive=True, height=300\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"<p style='font-size: 0.9em;'>Upload the foreground image where the polygon will be selected.</p>\"\n",
    "            )\n",
    "            gr.Markdown(\"### Foreground Image with Polygon\")\n",
    "            foreground_image_with_polygon = gr.Image(\n",
    "                label=\"\", type=\"pil\", interactive=True, height=300\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"<p style='font-size: 0.9em;'>Click on the image to define the polygon area. After selecting at least three points, click <strong>Close Polygon</strong>.</p>\"\n",
    "            )\n",
    "            close_polygon_button = gr.Button(\"Close Polygon\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Background Image\")\n",
    "            background_image = gr.Image(\n",
    "                label=\"\", type=\"pil\", interactive=True, height=300\n",
    "            )\n",
    "            gr.Markdown(\"<p style='font-size: 0.9em;'>Upload the background image where the polygon will be placed.</p>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Background Image with Polygon Overlay\")\n",
    "            background_image_with_polygon = gr.Image(\n",
    "                label=\"\", type=\"pil\", height=500\n",
    "            )\n",
    "            gr.Markdown(\"<p style='font-size: 0.9em;'>Adjust the position of the polygon using the sliders below.</p>\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Blended Image\")\n",
    "            output_image = gr.Image(\n",
    "                label=\"\", type=\"pil\", height=500  # Increased height for larger display\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            dx = gr.Slider(\n",
    "                label=\"Horizontal Offset\", minimum=-500, maximum=500, step=1, value=0\n",
    "            )\n",
    "        with gr.Column():\n",
    "            dy = gr.Slider(\n",
    "                label=\"Vertical Offset\", minimum=-500, maximum=500, step=1, value=0\n",
    "            )\n",
    "        blend_button = gr.Button(\"Blend Images\")\n",
    "\n",
    "    # Interactions\n",
    "\n",
    "    # Copy the original image to the interactive image when uploaded\n",
    "    foreground_image_original.change(\n",
    "        fn=lambda img: img,\n",
    "        inputs=foreground_image_original,\n",
    "        outputs=foreground_image_with_polygon,\n",
    "    )\n",
    "\n",
    "    # User interacts with the image with polygon\n",
    "    foreground_image_with_polygon.select(\n",
    "        add_point,\n",
    "        inputs=[foreground_image_original, polygon_state],\n",
    "        outputs=[foreground_image_with_polygon, polygon_state],\n",
    "    )\n",
    "\n",
    "    close_polygon_button.click(\n",
    "        fn=close_polygon_and_reset_dx,\n",
    "        inputs=[foreground_image_original, polygon_state, dx, dy, background_image_original],\n",
    "        outputs=[foreground_image_with_polygon, polygon_state, background_image_with_polygon, dx],\n",
    "    )\n",
    "\n",
    "    background_image.change(\n",
    "        fn=lambda img: img,\n",
    "        inputs=background_image,\n",
    "        outputs=background_image_original,\n",
    "    )\n",
    "\n",
    "    # Update background image when dx or dy changes\n",
    "    dx.change(\n",
    "        fn=update_background,\n",
    "        inputs=[background_image_original, polygon_state, dx, dy],\n",
    "        outputs=background_image_with_polygon,\n",
    "    )\n",
    "    dy.change(\n",
    "        fn=update_background,\n",
    "        inputs=[background_image_original, polygon_state, dx, dy],\n",
    "        outputs=background_image_with_polygon,\n",
    "    )\n",
    "\n",
    "    # Blend images when button is clicked\n",
    "    blend_button.click(\n",
    "        fn=blending,\n",
    "        inputs=[foreground_image_original, background_image_original, dx, dy, polygon_state],\n",
    "        outputs=output_image,\n",
    "    )\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "# fg_np_trial = np.array(cv2.cvtColor(cv2.imread('source.png'), cv2.COLOR_BGR2RGB))\n",
    "# bg_np_trial = np.array(cv2.cvtColor(cv2.imread('target.png'), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# print('fg_np_trial shape:', fg_np_trial.shape)\n",
    "# fg_img_tensor_trial = torch.from_numpy(fg_np_trial).to(device).permute(2, 0, 1).unsqueeze(0).float()\n",
    "# bg_img_tensor_trial = torch.from_numpy(bg_np_trial).to(device).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "# # print('fg tensor shape:', fg_img_tensor_trial.shape)\n",
    "\n",
    "# mask_bg_trial = create_mask_from_points(np.array([(0, 0), (0, 200), (200, 200), (200, 0)]), 506, 416)\n",
    "# mask_fg_trial = create_mask_from_points(np.array([(0, 0), (0, 200), (200, 0), (200, 200)]), 506, 416)\n",
    "# # plt.imshow(mask_trial,cmap='gray')\n",
    "# # plt.show()\n",
    "# # print(mask_trial)\n",
    "# mask_bg_tensor_trial = torch.from_numpy(mask_bg_trial).to('cpu').unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "# mask_fg_tensor_trial = torch.from_numpy(mask_fg_trial).to('cpu').unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "\n",
    "# f_trial = bg_img_tensor_trial * mask_bg_tensor_trial  # Masked blended image\n",
    "# g_trial = fg_img_tensor_trial * mask_fg_tensor_trial  # Masked foreground image\n",
    "\n",
    "# # Compute the difference between masked images (g_tude)\n",
    "# g_tude_trial = f_trial - g_trial\n",
    "\n",
    "# # print(g_tude_trial.shape)\n",
    "# fg_img_trial_np = fg_img_tensor_trial[0].permute(1, 2, 0).to('cpu').numpy()\n",
    "# f_trial_np = f_trial[0].permute(1, 2, 0).to('cpu').numpy()\n",
    "# g_tude_np = g_tude_trial[0].permute(1, 2, 0).to('cpu').numpy()\n",
    "\n",
    "# print('fg_img_trial_np shape:', fg_img_trial_np.shape)\n",
    "# print(fg_img_trial_np,-fg_np_trial)\n",
    "# # plt.imshow(fg_img_trial_np)\n",
    "# # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the Laplacian loss between the foreground and blended image\n",
    "# def cal_laplacian_loss(foreground_img, foreground_mask, blended_img, background_mask):\n",
    "#     \"\"\"\n",
    "#     Computes the Laplacian loss between the foreground and blended images within the masks.\n",
    "\n",
    "#     Args:\n",
    "#         foreground_img (torch.Tensor): Foreground image tensor.\n",
    "#         foreground_mask (torch.Tensor): Foreground mask tensor.\n",
    "#         blended_img (torch.Tensor): Blended image tensor.\n",
    "#         background_mask (torch.Tensor): Background mask tensor.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: The computed Laplacian loss.\n",
    "#     \"\"\"\n",
    "#     loss = torch.tensor(0.0, device=foreground_img.device)\n",
    "#     ### FILL: Compute Laplacian Loss with https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html.\n",
    "#     ### Note: The loss is computed within the masks.\n",
    "\n",
    "#     laplacian_kernel = torch.tensor([[[[0, 1, 0], \n",
    "#                                        [1, -4, 1], \n",
    "#                                        [0, 1, 0]]]], dtype=torch.float32, device=foreground_img.device)\n",
    "\n",
    "#     laplacian_kernel = laplacian_kernel.repeat(3,1,1,1)\n",
    "\n",
    "#     # Apply the masks to the images\n",
    "#     f_tude = torch.zeros_like(blended_img, device=foreground_img.device)\n",
    "#     blended_area = torch.zeros_like(background_mask, device=foreground_img.device)\n",
    "#     foreground_mask_expand = foreground_mask.bool().expand(-1, 3, -1, -1)\n",
    "#     background_mask_expand = background_mask.bool().expand(-1, 3, -1, -1)\n",
    "    \n",
    "#     f_tude[background_mask_expand] = blended_img[background_mask_expand] - foreground_img[foreground_mask_expand]\n",
    "#     # Compute the difference between masked images (g_tude)\n",
    "\n",
    "\n",
    "#     # Apply Laplacian convolution on the difference\n",
    "#     laplacian_f_tude = F.conv2d(f_tude, laplacian_kernel, padding=1, groups=foreground_img.shape[1])\n",
    "\n",
    "#     loss = laplacian_f_tude.sum()\n",
    "\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# # Perform Poisson image blending\n",
    "# def blending(foreground_image_original, background_image_original, dx, dy, polygon_state):\n",
    "#     \"\"\"\n",
    "#     Blends the foreground polygon area onto the background image using Poisson blending.\n",
    "\n",
    "#     Args:\n",
    "#         foreground_image_original (PIL.Image): The original foreground image.\n",
    "#         background_image_original (PIL.Image): The original background image.\n",
    "#         dx (int): Horizontal offset.\n",
    "#         dy (int): Vertical offset.\n",
    "#         polygon_state (dict): The current state of the polygon.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: The blended image as a numpy array.\n",
    "#     \"\"\"\n",
    "#     if not polygon_state['closed'] or background_image_original is None or foreground_image_original is None:\n",
    "#         return background_image_original  # Return original background if conditions are not met\n",
    "\n",
    "#     # Convert images to numpy arrays\n",
    "#     foreground_np = np.array(foreground_image_original)\n",
    "#     background_np = np.array(background_image_original)\n",
    "\n",
    "#     # Get polygon points and shift them by dx and dy\n",
    "#     foreground_polygon_points = np.array(polygon_state['points']).astype(np.int64)\n",
    "#     background_polygon_points = foreground_polygon_points + np.array([int(dx), int(dy)]).reshape(1, 2)\n",
    "\n",
    "#     # Create masks from polygon points\n",
    "#     foreground_mask = create_mask_from_points(foreground_polygon_points, foreground_np.shape[0], foreground_np.shape[1])\n",
    "#     background_mask = create_mask_from_points(background_polygon_points, background_np.shape[0], background_np.shape[1])\n",
    "\n",
    "#     # Convert numpy arrays to torch tensors\n",
    "#     device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # Using CPU will be slow\n",
    "#     fg_img_tensor = torch.from_numpy(foreground_np).to(device).permute(2, 0, 1).unsqueeze(0).float() / 255.\n",
    "#     bg_img_tensor = torch.from_numpy(background_np).to(device).permute(2, 0, 1).unsqueeze(0).float() / 255.\n",
    "#     fg_mask_tensor = torch.from_numpy(foreground_mask).to(device).unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "#     bg_mask_tensor = torch.from_numpy(background_mask).to(device).unsqueeze(0).unsqueeze(0).float() / 255.\n",
    "\n",
    "#     # Initialize blended image\n",
    "#     blended_img = bg_img_tensor.clone()\n",
    "#     mask_expanded = bg_mask_tensor.bool().expand(-1, 3, -1, -1)\n",
    "#     blended_img[mask_expanded] = blended_img[mask_expanded] * 0.9 + fg_img_tensor[fg_mask_tensor.bool().expand(-1, 3, -1, -1)] * 0.1\n",
    "#     blended_img.requires_grad = True\n",
    "\n",
    "#     # Set up optimizer\n",
    "#     optimizer = torch.optim.Adam([blended_img], lr=5e-3)\n",
    "\n",
    "#     # Optimization loop\n",
    "#     iter_num = 10000\n",
    "#     for step in range(iter_num):\n",
    "#         blended_img_for_loss = blended_img.detach() * (1. - bg_mask_tensor) + blended_img * bg_mask_tensor  # Only blending in the mask region\n",
    "\n",
    "#         loss = cal_laplacian_loss(fg_img_tensor, fg_mask_tensor, blended_img_for_loss, bg_mask_tensor)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if step % 50 == 0:\n",
    "#             print(f'Optimize step: {step}, Laplacian distance loss: {loss.item()}')\n",
    "\n",
    "#         if step == (iter_num // 2): ### decrease learning rate at the half step\n",
    "#             optimizer.param_groups[0]['lr'] *= 0.1\n",
    "\n",
    "#         blended_img.data[~mask_expanded] = bg_img_tensor.clone().data[~mask_expanded]\n",
    "\n",
    "#     # Convert result back to numpy array\n",
    "#     result = torch.clamp(blended_img.detach(), 0, 1).cpu().permute(0, 2, 3, 1).squeeze().numpy() * 255\n",
    "#     result = result.astype(np.uint8)\n",
    "#     return result\n",
    "\n",
    "# # Helper function to close the polygon and reset dx\n",
    "# def close_polygon_and_reset_dx(img_original, polygon_state, dx, dy, background_image_original):\n",
    "#     \"\"\"\n",
    "#     Closes the polygon, resets dx to 0, and updates the background image.\n",
    "\n",
    "#     Args:\n",
    "#         img_original (PIL.Image): The original image.\n",
    "#         polygon_state (dict): The current state of the polygon.\n",
    "#         dx (int): Horizontal offset.\n",
    "#         dy (int): Vertical offset.\n",
    "#         background_image_original (PIL.Image): The original background image.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: Updated image with polygon, updated polygon state, updated background image, and reset dx value.\n",
    "#     \"\"\"\n",
    "#     # Close polygon\n",
    "#     img_with_poly, updated_polygon_state = close_polygon(img_original, polygon_state)\n",
    "\n",
    "#     # Reset dx value to 0\n",
    "#     new_dx = gr.update(value=0)\n",
    "\n",
    "#     # Update background image\n",
    "#     updated_background = update_background(background_image_original, updated_polygon_state, 0, dy)\n",
    "#     return img_with_poly, updated_polygon_state, updated_background, new_dx\n",
    "\n",
    "# # Gradio Interface\n",
    "# with gr.Blocks(title=\"Poisson Image Blending\", css=\"\"\"\n",
    "#     body {\n",
    "#         background-color: #1e1e1e;\n",
    "#         color: #ffffff;\n",
    "#     }\n",
    "#     .gr-button {\n",
    "#         font-size: 1em;\n",
    "#         padding: 0.75em 1.5em;\n",
    "#         border-radius: 8px;\n",
    "#         background-color: #6200ee;\n",
    "#         color: #ffffff;\n",
    "#         border: none;\n",
    "#     }\n",
    "#     .gr-button:hover {\n",
    "#         background-color: #3700b3;\n",
    "#     }\n",
    "#     .gr-slider input[type=range] {\n",
    "#         accent-color: #03dac6;\n",
    "#     }\n",
    "#     .gr-text, .gr-markdown {\n",
    "#         font-size: 1.1em;\n",
    "#     }\n",
    "#     .gr-markdown h1, .gr-markdown h2, .gr-markdown h3 {\n",
    "#         color: #bb86fc;\n",
    "#     }\n",
    "#     .gr-input, .gr-output {\n",
    "#         background-color: #2c2c2c;\n",
    "#         border: 1px solid #3c3c3c;\n",
    "#     }\n",
    "# \"\"\") as demo:\n",
    "#     # Initialize states\n",
    "#     polygon_state = gr.State(initialize_polygon())\n",
    "#     background_image_original = gr.State(value=None)\n",
    "\n",
    "#     # Title and description\n",
    "#     gr.Markdown(\"<h1 style='text-align: center;'>Poisson Image Blending</h1>\")\n",
    "#     gr.Markdown(\"<p style='text-align: center; font-size: 1.2em;'>Blend a selected area from a foreground image onto a background image with adjustable positions.</p>\")\n",
    "\n",
    "#     with gr.Row():\n",
    "#         with gr.Column():\n",
    "#             gr.Markdown(\"### Foreground Image\")\n",
    "#             foreground_image_original = gr.Image(\n",
    "#                 label=\"\", type=\"pil\", interactive=True, height=300\n",
    "#             )\n",
    "#             gr.Markdown(\n",
    "#                 \"<p style='font-size: 0.9em;'>Upload the foreground image where the polygon will be selected.</p>\"\n",
    "#             )\n",
    "#             gr.Markdown(\"### Foreground Image with Polygon\")\n",
    "#             foreground_image_with_polygon = gr.Image(\n",
    "#                 label=\"\", type=\"pil\", interactive=True, height=300\n",
    "#             )\n",
    "#             gr.Markdown(\n",
    "#                 \"<p style='font-size: 0.9em;'>Click on the image to define the polygon area. After selecting at least three points, click <strong>Close Polygon</strong>.</p>\"\n",
    "#             )\n",
    "#             close_polygon_button = gr.Button(\"Close Polygon\")\n",
    "#         with gr.Column():\n",
    "#             gr.Markdown(\"### Background Image\")\n",
    "#             background_image = gr.Image(\n",
    "#                 label=\"\", type=\"pil\", interactive=True, height=300\n",
    "#             )\n",
    "#             gr.Markdown(\"<p style='font-size: 0.9em;'>Upload the background image where the polygon will be placed.</p>\")\n",
    "\n",
    "#     with gr.Row():\n",
    "#         with gr.Column():\n",
    "#             gr.Markdown(\"### Background Image with Polygon Overlay\")\n",
    "#             background_image_with_polygon = gr.Image(\n",
    "#                 label=\"\", type=\"pil\", height=500\n",
    "#             )\n",
    "#             gr.Markdown(\"<p style='font-size: 0.9em;'>Adjust the position of the polygon using the sliders below.</p>\")\n",
    "#         with gr.Column():\n",
    "#             gr.Markdown(\"### Blended Image\")\n",
    "#             output_image = gr.Image(\n",
    "#                 label=\"\", type=\"pil\", height=500  # Increased height for larger display\n",
    "#             )\n",
    "\n",
    "#     with gr.Row():\n",
    "#         with gr.Column():\n",
    "#             dx = gr.Slider(\n",
    "#                 label=\"Horizontal Offset\", minimum=-500, maximum=500, step=1, value=0\n",
    "#             )\n",
    "#         with gr.Column():\n",
    "#             dy = gr.Slider(\n",
    "#                 label=\"Vertical Offset\", minimum=-500, maximum=500, step=1, value=0\n",
    "#             )\n",
    "#         blend_button = gr.Button(\"Blend Images\")\n",
    "\n",
    "#     # Interactions\n",
    "\n",
    "#     # Copy the original image to the interactive image when uploaded\n",
    "#     foreground_image_original.change(\n",
    "#         fn=lambda img: img,\n",
    "#         inputs=foreground_image_original,\n",
    "#         outputs=foreground_image_with_polygon,\n",
    "#     )\n",
    "\n",
    "#     # User interacts with the image with polygon\n",
    "#     foreground_image_with_polygon.select(\n",
    "#         add_point,\n",
    "#         inputs=[foreground_image_original, polygon_state],\n",
    "#         outputs=[foreground_image_with_polygon, polygon_state],\n",
    "#     )\n",
    "\n",
    "#     close_polygon_button.click(\n",
    "#         fn=close_polygon_and_reset_dx,\n",
    "#         inputs=[foreground_image_original, polygon_state, dx, dy, background_image_original],\n",
    "#         outputs=[foreground_image_with_polygon, polygon_state, background_image_with_polygon, dx],\n",
    "#     )\n",
    "\n",
    "#     background_image.change(\n",
    "#         fn=lambda img: img,\n",
    "#         inputs=background_image,\n",
    "#         outputs=background_image_original,\n",
    "#     )\n",
    "\n",
    "#     # Update background image when dx or dy changes\n",
    "#     dx.change(\n",
    "#         fn=update_background,\n",
    "#         inputs=[background_image_original, polygon_state, dx, dy],\n",
    "#         outputs=background_image_with_polygon,\n",
    "#     )\n",
    "#     dy.change(\n",
    "#         fn=update_background,\n",
    "#         inputs=[background_image_original, polygon_state, dx, dy],\n",
    "#         outputs=background_image_with_polygon,\n",
    "#     )\n",
    "\n",
    "#     # Blend images when button is clicked\n",
    "#     blend_button.click(\n",
    "#         fn=blending,\n",
    "#         inputs=[foreground_image_original, background_image_original, dx, dy, polygon_state],\n",
    "#         outputs=output_image,\n",
    "#     )\n",
    "\n",
    "# # Launch the Gradio app\n",
    "# demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
